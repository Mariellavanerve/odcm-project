{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sustainability in Materials and Product Prices H&M**(change after disvussing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scraper has been set up using the BeautifulSoup and Selenium library from python to gather information about the products of the Dutch H&M website. Furthermore, this information can be used to determine the correlation between the price and material of H&M products. This Jupyter notebook will cover the following three parts: \n",
    "\n",
    "1. The requirements for scraping\n",
    "2. The H&M webscraper and saving the scraped data in a json file \n",
    "3. The data inspection of the scraped H&M data\n",
    "\n",
    "The code will be described in this notebook, along with their function. The feasible sample size contains 7904 products for H&M Women items. \n",
    "\n",
    "NOTE: Due to possible revisions to the H&M website, the class names used to extract the data may have changed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Requirements for scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import libraries and packages\n",
    "\n",
    "We import the libraries required to run our code in the first cell of our notebook. For the following, these libraries are necessary: \n",
    "\n",
    "* The requests library will be used to retrieve the HTML content of the Dutch H&M website.\n",
    "* The BeautifulSoup library will be used to extract the product URLs to create a list of product URLs. \n",
    "* We will use the time library because we need time to click and open the buttons.  \n",
    "* The json library will be used to store the scraped data in a JSON file.\n",
    "* The random library is needed to shuffle the product list to get the URLs in random order. \n",
    "* The csv library will be used to store the product list with URLs in a csv file. \n",
    "* The selenium library will be used to extract the product information of the product pages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import random \n",
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "import selenium.webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Running requests\n",
    "\n",
    "This part of the web scraper contains code to scrape the H&M web page(s) of the H&M Women View All section (in Dutch: 'Dames, bekijk alle items')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "request = requests.get('https://www2.hm.com/nl_nl/dames/shop-by-product/view-all.html', headers = header)\n",
    "request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "soup = BeautifulSoup(request.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Collecting the page URLs\n",
    "\n",
    "The code below has been used to collect all of the product URLs. A list called page_urls contains all of the page URLs up to and including 9972 products. The amount of products is used by H&M in the page URL. In order to create the list of page URLs, we added a counter of 36 products per page to the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "page_urls = []\n",
    "while counter <= 9972:\n",
    "    page_urls.append(f'https://www2.hm.com/nl_nl/dames/shop-by-product/view-all.html?sort=stock&image-size=small&image=model&offset={counter}&page-size=36.html')\n",
    "    counter+=36\n",
    "page_urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Collecting the product URLs\n",
    "\n",
    "The product URLs of the first page are scraped by using BeautifulSoup. The following function generates the product URLs for the first page of the View All page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_producturls(soup):\n",
    "    producturl = []\n",
    "    for item in soup.find_all('li', class_=\"product-item\"):\n",
    "        firstclick= f'https://www2.hm.com'+ item.find('a')['href']\n",
    "        producturl.append(firstclick)\n",
    "\n",
    "    return producturl\n",
    "    \n",
    "get_producturls(soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather all of the product URLs of the 9972 available items in the Women View All section, we loop the function through all the different page urls that have been collected. These product URLs are added to the list: product_list. By printing the length of the product_list, we can see if it indeed includes all of the 9972 available URLs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_list = []\n",
    "for page_url in page_urls:\n",
    "    request = requests.get(page_url, headers=header)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    product_list.extend(get_producturls(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(product_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product URLs in the product_list have been randomly shuffled to create a random order of which we can take a random sample of the products in this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(product_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Saving the product URLs in a CSV file\n",
    "\n",
    "The complete list of product URLs has been saved in a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('productlistHM.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i, product_url in enumerate(product_list):\n",
    "        if i == 0:\n",
    "            continue  # skip the first row\n",
    "        writer.writerow([product_url])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: The H&M Webscraper\n",
    "\n",
    "Chapter 2 contains the code which has been used to scrape the H&M website and to save it as a JSON file.\n",
    "\n",
    "### 2.1 Running the chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Collection the product information.\n",
    "\n",
    "The following function has been used to collect the product information. The data collected for the items in the H&M Women's section includes the product title, the price, the color, the product URL, the current timestamp of scraping, the buitenlaag information, the material information and if the product is a new arrival product or that it is not a new arrival product. Selenium has been used to automatically accept the cookies to close any pop-ups. We can continue on to scraping after accepting the cookies. Moreover, selenium has been used to scrape this information and to find-, click on- and open the buttons for the material and new arrival information for the different products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "    # The cookies are automatically accepted to close pop-ups before proceeding with web scraping.\n",
    "    try:\n",
    "        button = driver.find_element(By.ID,\"onetrust-accept-btn-handler\")\n",
    "        button.click()\n",
    "    except:\n",
    "        button = ''\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "    # Extracting the title of the product being scraped\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME, \"ProductName-module--container__bmkk9\").text\n",
    "    except:\n",
    "        title = 'No title'  \n",
    "\n",
    "    print(title) \n",
    "    \n",
    "    # Extracting the price of the product being scraped\n",
    "    try:\n",
    "        price = driver.find_element(By.CLASS_NAME, \"Price-module--black-large__Fa6KP\").text\n",
    "    except:\n",
    "        price = 'No price'\n",
    "\n",
    "    print(price)\n",
    "    \n",
    "    # Extracting the color of the product being scraped\n",
    "    try:\n",
    "        color = driver.find_element(By.CLASS_NAME, \"product-input-label\").text\n",
    "    except:\n",
    "        color = 'No color'  \n",
    "\n",
    "    print(color)     \n",
    "\n",
    "    # Extracting the current timestamp of collecting the product information\n",
    "    timestamp = int(time.time())\n",
    "    print(f'{timestamp}')\n",
    "    \n",
    "    # Extracting the product URL of the product being scraped\n",
    "    print(f'{url}')\n",
    "    \n",
    "    time.sleep(10)\n",
    "\n",
    "    # Find the material information button element by its CSS selector\n",
    "    button = driver.find_element(By.ID,\"toggle-materialsAndSuppliersAccordion\")\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Click on the button\n",
    "    button.click()\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Set aria-expanded attribute to true\n",
    "    driver.execute_script(\"arguments[0].setAttribute('aria-expanded', 'true')\", button)\n",
    "\n",
    "    # Wait for the box to open \n",
    "    time.sleep(5)\n",
    "\n",
    "    # Extracting the buitenlaag of the product being scraped\n",
    "    try:\n",
    "        buitenlaag = driver.find_element(By.CLASS_NAME, \"f94b22\").text\n",
    "    except:\n",
    "        buitenlaag = 'No buitenlaag' \n",
    "\n",
    "    print(buitenlaag)\n",
    "\n",
    "    # Extracting the material of the product being scraped\n",
    "    try:\n",
    "        material = driver.find_element(By.CLASS_NAME, \"f0d614\").text \n",
    "    except:\n",
    "        material = 'No material'  \n",
    "\n",
    "    print(material)\n",
    "\n",
    "    # Find the description button element by its CSS selector\n",
    "    button = driver.find_element(By.ID,\"toggle-descriptionAccordion\")\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Click on the button\n",
    "    button.click()\n",
    "    \n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Set aria-expanded attribute to true\n",
    "    driver.execute_script(\"arguments[0].setAttribute('aria-expanded', 'true')\", button)\n",
    "\n",
    "    # Wait for the box to open \n",
    "    time.sleep(3)\n",
    "\n",
    "    # Extracting if the product being scraped is a new arrival product\n",
    "    try:\n",
    "        newarrival = driver.find_element(By.CLASS_NAME, \"fccfcd\").text\n",
    "    except:\n",
    "        newarrival = 'No New Arrival'  \n",
    "\n",
    "    print(newarrival)\n",
    "\n",
    "    my_data = {'title': title,\n",
    "    'price': price,\n",
    "    'color': color,\n",
    "    'timestamp': timestamp,\n",
    "    'url': url,\n",
    "    'buitenlaag': buitenlaag,\n",
    "    'material': material,\n",
    "    'newarrival': newarrival\n",
    "    }\n",
    "    \n",
    "    return(my_data)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Looping through the products and writing the collected data to a JSON file \n",
    "\n",
    "The following code loops over every product URL which has been saved in the CSV file to collect the product information for the products in the file. The collected data has been written into a JSON file where it has been saved. When an error occured during the scraping process, the matching product has not been saved to the JSON file. In this way only products are saved, of which it was possible to scrape all of the product information without error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('productlistHM.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        url = row[0]\n",
    "        try:\n",
    "            my_data = scrape(url)\n",
    "            with open('HM_product_information.json', 'a', encoding='utf-8') as f:\n",
    "                f.write(json.dumps(my_data))\n",
    "                f.write('\\n')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Data inspection\n",
    "The output from the web scraper is stored in a JSON file. RStudio has been used to inspect the data. To import the data in RStudio, the dataset has been converted to a CSV file by using python code. Furthermore, the dataset is cleaned so that it may be utilized to identify shared characteristics and distinctive features among the various products. The python code used to convert the dataset and save it as a CSV file is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('final_combined_HM.json') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        try:\n",
    "            data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Could not parse line: {line}\")\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('HM_products.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
